{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 深層学習スクラッチ 畳み込みニューラルネットワーク2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tominagashuuji/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(X_train, t_train), (X_test, t_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "t_train_one_hot = enc.fit_transform(t_train[:, np.newaxis])\n",
    "t_test_one_hot = enc.fit_transform(t_test[:,  np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】2次元畳み込み層の作成¶\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "フォワードプロパゲーションの数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \n",
    "    def __init__(self, initializer, out_chanel, in_chanel, height, width, optimizer):\n",
    "        init = initializer\n",
    "        self.w = init.W(out_chanel, in_chanel, height, width)\n",
    "        self.b = init.B(out_chanel)\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.sample_size, self.in_chanel, self.x_height, self.x_width = X.shape\n",
    "        self.out_chanel, self.inchanel, self.w_height, self.w_width = self.w.shape\n",
    "        self.XB = X\n",
    "        \n",
    "        \n",
    "        A  = np.zeros([self.sample_size,  self.out_chanel, self.x_height -2, self.x_width -2])\n",
    "        for n in range(self.sample_size):\n",
    "            for outchan in range(self.out_chanel):\n",
    "                for inchan in range(self.in_chanel):\n",
    "                    for i in range(self.x_height-2):\n",
    "                        for j in range(self.x_width-2):\n",
    "                            sig = 0\n",
    "                            for s in range(self.w_height):\n",
    "                                for t in range(self.w_width):\n",
    "                                    sig += X[n, inchan, i+s, j+t] * self.w[outchan, inchan, s, t]\n",
    "                        A[n, outchan, i, j] += sig + self.b[outchan]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        n_out_h , n_out_w = N_out(self.x_height, self.x_width, 0, self.w_height, self.w_width, 1)\n",
    "        self.lb = dA.sum(axis=(0, 2, 3))\n",
    "        \n",
    "        self.lw = np.zeros_like(self.w)\n",
    "        for n in range(self.sample_size):\n",
    "            for m in range(self.out_chanel):\n",
    "                for k in range(self.in_chanel):\n",
    "                    for s in range(self.w_height):\n",
    "                        for t in range(self.w_width):\n",
    "                            for i in range(self.w_height-1):\n",
    "                                for j in range(self.w_width-1):\n",
    "                                    self.lw[m, k, s, t] += dA[n, m, i, j] * self.XB[n, k, i+s, j+t]\n",
    "        \n",
    "        \n",
    "        dZ = np.zeros_like(self.XB)\n",
    "        for n in range(self.sample_size):\n",
    "            for m in range(self.out_chanel):\n",
    "                for k in range(self.inchanel):\n",
    "                    for i in range(self.x_height):\n",
    "                        for j in range(self.x_width):\n",
    "                            sig = 0\n",
    "                            for s in range(self.w_height):\n",
    "                                for t in range(self.w_width):\n",
    "                                    if i-s<0 or i-s>n_out_h-1 or j-t < 0 or j-t>n_out_w-1:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        sig += dA[n, m, i-s, j-t] * self.w[m, k, s, t]\n",
    "                            dZ[n, k, i, j] += sig\n",
    "        \n",
    "        \n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】2次元畳み込み後の出力サイズ¶\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out(Nh_in, Nw_in, P, Fh, Fw, S):\n",
    "    Nh_out = ((Nh_in  + 2*P - Fh) / S) + 1\n",
    "    Nw_out = ((Nw_in + 2*P-Fw) / S) + 1\n",
    "    return int(Nh_out), int(Nw_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    pooling層のクラス\n",
    "    2*2の正方行列内の最大値をリストに格納\n",
    "    リストをreshapeすることで行列として返す\n",
    "    \n",
    "    2*2の正方行列内の最大値のインデックスを縦横それぞれ別のリストに格納\n",
    "    要素が0の行列を作り、backwardで帰ってきた値を最大値のインデックスがあった場所へ代入\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,A):\n",
    "        self.AB = A\n",
    "        pooling_list = []\n",
    "        self.h_list = []\n",
    "        self.w_list = []\n",
    "        for n in range(A.shape[0]):\n",
    "            for cha in range(A.shape[1]):\n",
    "                for h in range(0, 26, 2):\n",
    "                    for w in range(0, 26, 2):\n",
    "                        pooling_list = np.append(pooling_list, np.max(A[n, cha, h:h+2, w:w+2]))\n",
    "                        self.h_list.append(np.unravel_index(np.argmax(A[n, cha, h:h+2, w:w+2]), A[n, cha, h:h+2, w:w+2].shape)[0])\n",
    "                        self.w_list.append(np.unravel_index(np.argmax(A[n, cha, h:h+2, w:w+2]), A[n, cha, h:h+2, w:w+2].shape)[1])\n",
    "                        \n",
    "        pooing = pooling_list.reshape(A.shape[0], A.shape[1], 13, 13)\n",
    "        return pooing\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        d = np.zeros_like(self.AB)\n",
    "        i = 0\n",
    "        for n in range(dA.shape[0]):\n",
    "            for cha in range(dA.shape[1]):\n",
    "                for h in range(0, 26, 2):\n",
    "                    for w in range(0, 26, 2):\n",
    "                        d[n, cha, h:h+2, w:w+2][self.h_list[i]][self.w_list[i]] = dA.reshape(-1)[i]\n",
    "                        i+=1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, X):\n",
    "        self.X_shape = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def bakcward(self, dA):\n",
    "        return dA.reshape(self.X_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。\n",
    "\n",
    "#### クラス\n",
    "#### FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        init = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.w = init.W(n_nodes1, n_nodes2)\n",
    "        self.b = init.B(n_nodes2)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.z = X\n",
    "        self.a = X@self.w + self.b\n",
    "        \n",
    "        return self.a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA @ self.w.T\n",
    "        self.lw = self.z.T @ dA\n",
    "        self.lb = np.sum(dA, axis=0)\n",
    "        \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer_cnn:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, out_chanel, in_chanel, height, width):\n",
    "\n",
    "        W = self.sigma * np.random.randn(out_chanel, in_chanel, height, width)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, out_chanel):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(out_chanel)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \n",
    "        layer.w = layer.w -  self.lr * layer.lw\n",
    "        layer.b = layer.b - self.lr*layer.lb\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_a = np.exp(A)\n",
    "        softmax_result = np.empty((A.shape[0], A.shape[1]))\n",
    "        exp_sum = np.sum(exp_a, axis=1)\n",
    "        for i in range(A.shape[0]):\n",
    "            softmax_result[i] = exp_a[i] / exp_sum[i]\n",
    "            \n",
    "        return softmax_result\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        \n",
    "        L_A = Z - Y\n",
    "        self.cross_entropy = -np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "        \n",
    "        \n",
    "        return L_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        \n",
    "        return Z * np.maximum(np.sign(self.A), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スクラッチが悪いせいで時間がかかるため、データ数を減らす\n",
    "X_train_min = X_train[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_min = t_train_one_hot[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_min = X_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_min = t_test_one_hot[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = Conv2d(SimpleInitializer_cnn(0.01), 3, 1, 3, 3, SGD(0.1))\n",
    "maxpool = MaxPool2D()\n",
    "relu = Relu()\n",
    "fc1 = FC(507, 10, SimpleInitializer(0.01), SGD(0.1))\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = conv2d.forward(X_train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = relu.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2 = maxpool.forward(A_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_3 = flat.forward(A_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_4 = fc1.forward(A_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_5 = softmax.forward(A_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_1 = softmax.backward(A_5, t_train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_2 = fc1.backward(dA_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_3 = flat.bakcward(dA_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_4 = maxpool.backward(dA_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_5 = relu.backward(dA_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_6 = conv2d.backward(dA_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = conv2d.forward(X_test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_2 = relu.forward(test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_3 = maxpool.forward(test_A_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_4 = flat.forward(test_A_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_5 = fc1.forward(test_A_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_6 = softmax.forward(test_A_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(test_A_6 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.134\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(t_test[:500], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "入力サイズ : 144×144, 3チャンネル  \n",
    "フィルタサイズ : 3×3, 6チャンネル  \n",
    "ストライド : 1  \n",
    "パディング : なし  \n",
    "出力サイズ：(6, 142, 142)  \n",
    "パラメータ数:168  \n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "入力サイズ : 60×60, 24チャンネル  \n",
    "フィルタサイズ : 3×3, 48チャンネル  \n",
    "ストライド　: 1  \n",
    "パディング : なし  \n",
    "出力サイズ：(48, 58,58)  \n",
    "パラメータ数:10416  \n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "入力サイズ : 20×20, 10チャンネル  \n",
    "フィルタサイズ: 3×3, 20チャンネル  \n",
    "ストライド : 2  \n",
    "パディング : なし  \n",
    "出力サイズ：(20, 9,9)  \n",
    "パラメータ数:1820  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
