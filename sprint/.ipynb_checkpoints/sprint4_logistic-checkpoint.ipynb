{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint4 機械学習スクラッチ ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初に使用予定のデータやライブラリーを宣言しておく\n",
    "import os # ファイルの読み込み先のpath指定する時に必要\n",
    "\n",
    "import numpy as np # numpyのライブラリー読み込み\n",
    "import pandas as pd # pandasのライブラリー読み込み\n",
    "\n",
    "import matplotlib.pyplot as plt # グラフ描写のライブラリー\n",
    "import matplotlib.patches as mpatches # 可視化\n",
    "import seaborn as sns # グラフ描写のライブラリー\n",
    "from matplotlib.colors import ListedColormap # マップ\n",
    "\n",
    "from sklearn.datasets import load_iris # アイリスデータ\n",
    "\n",
    "from sklearn.model_selection import train_test_split # データ分割\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # 標準化\n",
    "from sklearn.neighbors import KNeighborsClassifier # 最近傍法\n",
    "\n",
    "from sklearn.metrics import accuracy_score # 正解率\n",
    "from sklearn.metrics import precision_score # 適合率\n",
    "from sklearn.metrics import recall_score # 再現率\n",
    "from sklearn.metrics import f1_score # F値\n",
    "from sklearn.metrics import confusion_matrix # 混合行列\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression # ロジスティック回帰\n",
    "from sklearn.svm import SVC #SVC \n",
    "from sklearn.model_selection import cross_val_score # 決定木\n",
    "from sklearn.tree import DecisionTreeClassifier # 決定木\n",
    "from sklearn.ensemble import RandomForestClassifier # ランダムフォレスト1\n",
    "from sklearn.datasets import make_classification # ランダムフォレスト2\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # 線形回帰モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # load_iris を dataという変数に保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = verbose\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.theta = 0\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "\n",
    "        self.sigmoid(self.liner_regression(X))\n",
    "        \n",
    "        self._gradient_descent(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        self.predict_proba(self,X)\n",
    "        return\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        self.sigmoid(self.liner_regression(self._gradient_descent(X)))\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return (1/(1 + np.exp(-z)))\n",
    "\n",
    "    def _linear_hypothesis(self, X): # リニアハイポセシス\n",
    "        linear_hypothesis = np.dot(X, self.theta)\n",
    "        return linear_hypothesis\n",
    "\n",
    "#     def liner_regression(self, x):\n",
    "#         theta = np.array([[-1, 2, 3], [4, -5, 6], [7, 8, -9]])\n",
    "#         return theta.T@x\n",
    "    \n",
    "## 最初自分が作成したコード\n",
    "#     def _gradient_descent(self, x):\n",
    "#         gd = 0\n",
    "#         # 内積使うとfor文不要\n",
    "#         for i in x.shape[0]: # m は（x.shape[0]）入力データの個数\n",
    "#             gd += (self.sigmoid(self.liner_regression(x)) - y[i])*x[i][j]\n",
    "#         gd = gd / x.shape[0]\n",
    "#         if x.shape[1] != 0: # j は 特徴量の入力値の列数を数えたい\n",
    "#             gd = gd + (ramda/x.shape[0])*theta[j]\n",
    "        \n",
    "#         alpha = 0.01\n",
    "#         theta_j = theta_j - (alpha * gd)\n",
    "        \n",
    "#         return gd\n",
    "\n",
    "## 線形回帰から持って来た奴\n",
    "#     def _gradient_descent(self, X, y):\n",
    "#         m = len(X)\n",
    "#         alpha = 0.01\n",
    "#         h = self._linear_hypothesis(X)\n",
    "\n",
    "#         gradient = np.dot((h - y), X)\n",
    "#         self.theta = self.theta - alpha/m*gradient\n",
    "#         return self.theta\n",
    "\n",
    "    def _gradient_descent(self, X):\n",
    "        shu = \"aaa\"\n",
    "        m = len(X)\n",
    "        y = np.arange(40).reshape(10,4) \n",
    "        xj = 0.5\n",
    "\n",
    "        print(m)\n",
    "        print(y)\n",
    "        print(xj)\n",
    "        j = 1/m(self._linear_hypothesis(X) - y) * xj\n",
    "        if j >= 1:\n",
    "            j = j + ramda/m*theta\n",
    "\n",
    "        return shu, j\n",
    "    \n",
    "    def tomi(self, X):\n",
    "        tomi = \"tomi\"\n",
    "        print(tomi)\n",
    "        return tomi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n",
    "ロジスティック回帰の仮定関数は、線形回帰の仮定関数を シグモイド関数 に通したものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310585786300049\n"
     ]
    }
   ],
   "source": [
    "# e = 1 # e は exp の事。exp は log の逆の事。対数と呼ぶ。 \n",
    "z = 1\n",
    "# gz = 1/(1 + e**(-z))　# これは違うw、eは対数だからそれに沿って計算する必要がある。\n",
    "e = np.exp(-z)\n",
    "gz = 1/(1 + e)\n",
    "print(gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4).reshape(4,1) # 特徴量4,1\n",
    "X = np.arange(40).reshape(10,4) # サンプル数10,特徴量4の行列\n",
    "\n",
    "slr = ScratchLogisticRegression(bias=False, num_iter=1000, lr=0.001, verbose=0)\n",
    "slr.tomi(X)\n",
    "\n",
    "slr.sigmoid(slr._linear_hypothesis(X))\n",
    "# slr.sigmoid(X)\n",
    "\n",
    "# y_slr_pred = slr.predict(X_test_std)\n",
    "# print(\"切片:\", slr.theta[0])\n",
    "# print(\"パラメータ：\", slr.theta[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66  34 -42]\n",
      " [ 34  93 -96]\n",
      " [-42 -96 126]]\n"
     ]
    }
   ],
   "source": [
    "shita = np.array([[-1, 2, 3], [4, -5, 6], [7, 8, -9]])\n",
    "x = np.array([[-1, 2, 3], [4, -5, 6], [7, 8, -9]])\n",
    "h0 = shita.T@x\n",
    "print(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 5.74952226e-19]\n",
      " [1.00000000e+00 1.00000000e+00 2.03109266e-42]\n",
      " [5.74952226e-19 2.03109266e-42 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "hx = 1/(1 + np.exp(h0*(-1)))\n",
    "print(hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。\n",
    "以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _gradient_descent():\n",
    "#     shu = \"aaa\"\n",
    "#     m = len(X)\n",
    "#     xj = 0.5\n",
    "#     j = 1/m(self._linear_hypothesis(self, X) - y)* xj\n",
    "#     if j >= 1:\n",
    "#         j = j + ramda/m*theta\n",
    "\n",
    "#     pass\n",
    "#     return shu, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shu, j = _gradient_descent()\n",
    "# print(shu)\n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]\n",
      " [32 33 34 35]\n",
      " [36 37 38 39]]\n",
      "10\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]\n",
      " [32 33 34 35]\n",
      " [36 37 38 39]]\n",
      "0.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-459851bd2aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mslr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-c2e159bcd1cf>\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linear_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mramda\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "X = np.arange(40).reshape(10,4) # サンプル数10,特徴量4の行列\n",
    "print(X)\n",
    "\n",
    "slr._gradient_descent(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 \n",
    "hθ(x) の出力がpredict_probaの返り値、さらにその値に閾値を設けて1と0のラベルとしたものがpredictの返り値となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大上さんとペアプロ\n",
    "python 機械学習プログラミング66p を使用したロジスティック回帰の動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt # グラフ描写のライブラリー\n",
    "import matplotlib.patches as mpatches # 可視化\n",
    "import seaborn as sns # グラフ描写のライブラリー\n",
    "from matplotlib.colors import ListedColormap # マップ\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tempfile import TemporaryFile\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris # sklearn.datasets を load_iris という変数で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicRegressionGD(object):\n",
    "    \"\"\"\n",
    "    パラメタ\n",
    "    eta:float 学習率\n",
    "    n_iter:int トレーニング回数\n",
    "    ramdom_state:int 重みを初期化する乱数シード\n",
    "\n",
    "    属性\n",
    "    w_１次配列　適合後の重み\n",
    "    cost_ : リスト　各エポックでの誤差平方和コスト関数\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,eta = 0.05,n_iter = 100,random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        パラメタ\n",
    "        X 配列　形は{n_sample,n_featuers}\n",
    "        サンプルはサンプルの個数でようはトレーニングデータ\n",
    "        y 配列のようなデータ構造、shape = {n_sample}\n",
    "        目的変数\n",
    "\n",
    "        戻り値\n",
    "        self:object\n",
    "        \"\"\"\n",
    "        # np.random.randomstateで同じ乱数になるようにシードで固定し、rgenにインスタンス化\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        #normalで正規分布に従う乱数を作成shape[1]は列の数\n",
    "        self.w_ = rgen.normal(loc=0.0,scale=0.01,size=1+X.shape[1])\n",
    "       # print(self.w_)\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            erros = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(erros)\n",
    "            self.w_[0] += self.eta * erros.sum()\n",
    "            #\n",
    "            cost = -y.dot(np.log(output)) -((1-y).dot(np.log(1-output)))\n",
    "            #costを追加する\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def net_input(self,X):\n",
    "        #総入力を計算\n",
    "        return np.dot(X,self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self,z):\n",
    "        return 1. /(1. + np.exp(-np.clip(z,-250,250)))\n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.where(self.net_input(X) >= 0.0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # load_iris\n",
    "\n",
    "#df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)\n",
    "X = iris.data[:,[0,3]]\n",
    "y = iris.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "\n",
    "X_train_01_subset = X_train[(y_train == 1) | (y_train ==0)]\n",
    "y_train_01_subset = y_train[(y_train == 1) | (y_train ==0)]\n",
    "\n",
    "lrgd = LogicRegressionGD(eta=0.05,n_iter=100,random_state=1)\n",
    "lrgd.fit(X_train_01_subset,y_train_01_subset)\n",
    "\n",
    "plot_decision_regions(X_train_01_subset,y_train_01_subset,clf=lrgd)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
