{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint15 論文読解入門\n",
    "## 1.このSprintについて\n",
    "### Sprintの目的\n",
    "機械学習分野の論文から有益な情報を引き出せるようにする\n",
    "これまで扱ってきた領域の論文から新たな知識を得る\n",
    "### どのように学ぶか\n",
    "ある論文に対しての問題に答えていくことで、読むポイントを学んでいきます。\n",
    "## 2.論文読解\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "### 問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "### 条件\n",
    "答える際は論文のどの部分からそれが分かるかを書く。\n",
    "\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回答:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 物体検出の分野にはどういった手法が存在したか。\n",
    "SPPnet [1]やFast R-CNN [2]という手法が存在していた。\n",
    "\n",
    "他にも R-CNN、overfeatというのがあった。今回のFasterR-CNN以降にも技術が存在している。\n",
    "\n",
    ">引用　1 INTRODUCTION より\n",
    "\n",
    ">Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck.\n",
    "\n",
    "<img src=\"物体検出.png\" width=\"600\">\n",
    "> 参考 https://note.mu/seishin55/n/n542b2b682721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "参考：https://tech-blog.abeja.asia/entry/object-detection-summary\n",
    "```\n",
    "Fast_R_CNNでは１枚の画像処理に2.3秒かかるが、そのうちRegionProposalで2秒を費やしていた。\n",
    "理由はFastR-CNNではRegion Proposalに従来技術であるSelective Search を使用していたのでそれが速度ボトルネックになっていた。\n",
    "Faster R-CNNはRegionProposalもCNN化する事で物体検出モデルを全てDNN化し、高速化する。\n",
    "他にも、Multi-task loss という学習技術を使っており、RegionProPosalモデルも込みでモデル全体をend-to-end で学習させることに成功している。\n",
    "```\n",
    "\n",
    "*論文参考場所：*\n",
    "```\n",
    "Faster R-CNN (NIPS 2015) [4] Fast R-CNNは、物体検出のほとんどの学習/識別フェーズをDeep Learningを用いることで実現できました。\n",
    "しかしながら、物体候補(Region Proposals)を検出するアルゴリズムは前述の論文同様、既存の手法を使っていました。 \n",
    "そこで、Faster R-CNNはRegion Proposal Network (RPN)という物体候補領域を推定してくれるネットワーク + RoI Poolingにクラス推定を行うことで\n",
    "End to Endで学習できるアーキテクチャを提案しました。\n",
    "```\n",
    "\n",
    "> 2 RELATED WORK の Figure 2: Faster R-CNN is a single, unified network\n",
    "> 論文参照：https://qiita.com/arutema47/items/8ff629a1516f7fd485f9\n",
    "\n",
    "<img src=\"Faster.png\" width=\"600\">\n",
    "\n",
    "\n",
    "> Fast_RCNN\n",
    "\n",
    "<img src=\"Fast_RCNN.png\" width=\"800\">\n",
    "\n",
    "> Faster_RCNN\n",
    "\n",
    "<img src=\"Faster_RCNN.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "大きく異なる場所は、mAP(%)が違う。mAP (mean Average Precision)<平均適合率の平均>高い方が良い。\n",
    "Two-Stageの方がOne-StageよりもmAP(%)が5%良い。\n",
    "(Two-Stage(58.7%)、One-Stage（53.8,9%））\n",
    "他にも差分はあるが詳細は表から参照出来る。\n",
    "\n",
    "> 引用 Table 10: One-Stage Detection vs. Two-Stage Proposal + Detection. \n",
    "\n",
    "<img src=\"One-Stage_vs_Two-Stage.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) RPNとは何か。\n",
    "物体領域提案ネットワーク （Region Proposal Network（RPN））の略。Faster-\n",
    "\n",
    "例えばRPNがやりたいことは, ネコっぽい領域を切り取ること。\n",
    "そのために必要なものは枠(領域) と それに対応するネコっぽさ(ネコ度)RPNはネコ度が高い枠を300ぐらい推定して次の処理へ送っています。\n",
    "\n",
    "![予め大きさの決めている枠（アンカー）で画像から猫っぽいところ抽出](cat01.png)\n",
    "![猫っぽいところの枠を集めた](cat02.png)\n",
    "\n",
    "RPNには二種類。\n",
    "* Fast R-CNN系: 大量の領域候補を持つ\n",
    "* Yolo/SSD系: 区間別に領域候補を持つ\n",
    "\n",
    "Fast R-CNN系は精度を高めるために用いられています.\n",
    "しかしながら, 実用には使えません(処理がおそすぎる)\n",
    "そのため, Yolo/SSD系が現在産業界では注目を集めています.\n",
    "しかしながら, 精度が少し劣るため,高い精度が要求されるシステムに実装は見送られています.さらなる研究が進むことが期待されています\n",
    "\n",
    "> 引用：1 INTRODUCTION＞\n",
    "To this end, we\n",
    "introduce novel Region Proposal Networks (RPNs) that\n",
    "share convolutional layers with state-of-the-art object\n",
    "detection networks [1], [2].\n",
    "\n",
    "> 引用：3 Region Proposal Networks＞\n",
    "which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\n",
    "\n",
    "> 参考：https://kazukilab.com/2018/08/09/post-1813/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) RoIプーリングとは何か。\n",
    "\n",
    "そもそもRoIとは？？\n",
    "```\n",
    "ROI（Region of Interest）とは、画像データのうち、操作の対象として選ぶ領域のことです。\n",
    "「対象領域」「注目領域」「関心領域」などともいいます。\n",
    "\n",
    "例えば、画像の一部分にだけ処理を施す場合にROIという言葉を使います。\n",
    "```\n",
    "> 参考：https://algorithm.joho.info/image-processing/roi/\n",
    "\n",
    "入力リストからすべての関心領域について、それに対応する入力特徴マップのセクションを取り、それを予め定義されたサイズ（例えば、7×7）にスケールします。スケーリングは次の方法で行います。\n",
    "\n",
    "領域候補を同じサイズのセクションに分割します（その数は出力の次元と同じ）各セクションで最大値を見つける\n",
    "これらの最大値を出力バッファにコピーする\n",
    "その結果、サイズの異なる長方形のリストから、固定サイズの対応する特徴マップのリストをすばやく取得できます。\n",
    "RoIプーリング出力の次元は、実際には入力特徴マップのサイズや領域提案のサイズに依存しないことに注意。領域候補を分割するセクションの数だけによって決定されます。\n",
    "\n",
    "RoIプーリングのメリットの1つは処理速度です。\n",
    "フレームに複数の物体候補がある場合（通常はたくさんある）、それらのすべてに対して同じ入力特徴マップを使用できます。\n",
    "ネットワーク処理の初期段階での畳み込み計算は非常にコストがかかるので、このアプローチは時間を大幅に節約できます。\n",
    "\n",
    "実際の動作を見てみましょう。今、8×8の単一の特徴マップの1つの関心領域に対して2×2の出力サイズでRoIプーリングを実行してみます。入力特徴マップは次のようになります。\n",
    "\n",
    ">参考：https://qiita.com/mshinoda88/items/9770ee671ea27f2c81a9\n",
    ">参考：https://lib-arts.hatenablog.com/entry/object_detection2\n",
    "\n",
    "```\n",
    "論文の和訳：\n",
    "非近似の合同トレーニング。説明したように上記で、RPNによって予測される境界ボックスは入力の機能も。\n",
    "RoIプーリング層[2] Fast R-CNNでは、畳み込み特性を受け入れます。\n",
    "また、入力として予測される境界ボックスも理論的に有効な逆伝播ソルバは勾配w.r.tも含まれます。\n",
    "```\n",
    "\n",
    ">引用：3.2 Sharing Features for RPN and Fast R-CNN＞\n",
    "The RoI pooling layer\n",
    "[2] in Fast R-CNN accepts the convolutional features\n",
    "and also the predicted bounding boxes as input, so\n",
    "a theoretically valid backpropagation solver should\n",
    "also involve gradients w.r.t. the box coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Anchorのサイズはどうするのが適切か。\n",
    "そもそもAnchorとは？？？\n",
    "```\n",
    "Anchorと呼ばれる領域を生成します。一言で言えば、物体の領域を示している矩形です。これらを画像内に一定間隔で生成します。これは後述のLoss Functionで利用されます。\n",
    "```\n",
    "どうするのが適切か？\n",
    "```\n",
    "Refineに使われる概念がAnchorである。物体検出において物体は3x3のような正方形であることは少ない。\n",
    "例えば人間はだいたい縦長であったり、車は横長で有ることが多い。このような図形情報を扱うために提案された概念がAnchorである。\n",
    "```\n",
    ">論文からの引用：  \n",
    "3.1.1 Anchors\n",
    "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is\n",
    "denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs\n",
    "2k scores that estimate probability of object or not object for each proposal4\n",
    ". The k proposals are parameterized relative to k reference boxes, which we callanchors. An anchor is centered at the sliding window\n",
    "in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and\n",
    "3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W Hk anchors in total.\n",
    "<img src=\"anchor.png\" img=\"100\">\n",
    "  \n",
    ">参考：  \n",
    "http://nonbiri-tereka.hatenablog.com/entry/2018/03/07/075835  \n",
    "https://qiita.com/arutema47/items/8ff629a1516f7fd485f9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "何というデータセットを使っているか？それぞれは何か？\n",
    "```\n",
    "PASCAL VOC 2007,2012とMS COCOデータセットを使っている。\n",
    "\n",
    "PASCAL VOC 2007,2012は、現実的なシーンの多数のビジュアルオブジェクトクラスからオブジェクトを認識することです\n",
    "（つまり、事前にセグメント化されたオブジェクトではありません）。\n",
    "これは基本的に、ラベル付き画像のトレーニングセットが提供されるという点で、教師付き学習学習問題です。選択された20のオブジェクトクラスは次のとおりです。\n",
    "\n",
    "人：人\n",
    "動物：鳥、猫、牛、犬、馬、羊\n",
    "車両：飛行機、自転車、ボート、バス、車、バイク、電車\n",
    "屋内：ボトル、椅子、ダイニングテーブル、鉢植え、ソファ、テレビ/モニター\n",
    "2つの主要な競技会と、2つの小規模な「テイスター」競技会があります。\n",
    "\n",
    "MS COCOデータセットは画像とキャプションのデータセット。１画像辺り５以下のキャプションをもつ。\n",
    "COCOは、大規模なオブジェクト検出、セグメンテーション、キャプションデータセットです。\n",
    "```\n",
    ">引用：  \n",
    "For the very deep VGG-16 model [3],\n",
    "our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\n",
    "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image.\n",
    "  \n",
    "\n",
    ">参考：  \n",
    "http://cocodataset.org/#home  \n",
    "https://qiita.com/arutema47/items/8ff629a1516f7fd485f9  \n",
    "\n",
    "\n",
    "先行研究に比べどういった指標値が得られているか？？\n",
    "```\n",
    "指標：\n",
    "物体検出の分野で重要とされている指標は、mAP (mean Average Precision) と FPS (Frame Per Second) 。\n",
    "つまり物体検出精度と処理速度のこと。\n",
    "結論としては論文の表が参考になる。\n",
    "論文内の表１１、１２からmAPがFast-RCNNを上回っている事が確認出来る。\n",
    "```\n",
    "\n",
    "<img src=\"Table11.png\" width=\"800\">\n",
    "<img src=\"Table12.png\" width=\"400\">\n",
    "\n",
    "```\n",
    "効率的かつ正確な地域提案生成のためのRPNを提示しました。\n",
    "たたみ込み機能をダウンストリーム検出ネットワークと共有することにより、地域提案のステップはほぼ無料です。この方法により、\n",
    "統合された深層学習ベースのオブジェクト検出システムをほぼリアルタイムのフレームレートで実行できます。\n",
    "学習したRPNは、領域提案の品質も向上させるため、全体的なオブジェクト検出精度も向上します。\n",
    "```\n",
    "\n",
    ">引用：  \n",
    "5 CONCLUSION\n",
    "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional\n",
    "features with the down-stream detection network, the region proposal step is nearly cost-free. Our method\n",
    "enables a unified, deep-learning-based object detection system to run at near real-time frame rates. The\n",
    "learned RPN also improves region proposal quality and thus the overall object detection accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
